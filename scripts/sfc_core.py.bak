#!/usr/bin/env python3
"""
SFC core runner with one-switch modes.
Usage examples (no long CLI):
  python scripts/sfc_core.py baseline
  python scripts/sfc_core.py fwtw
  python scripts/sfc_core.py graph
  python scripts/sfc_core.py refined
Optional: --config to point to a different YAML (defaults to config/proper_sfc_config.yaml)

Config file must contain an 'sfc' block with:
  base_dir, cache_dir, output_dir, date (YYYY-MM-DD),
  instrument_map, flow_map,
  fwtw (for fwtw mode),
  graph_spec (for graph mode),
  instrument_group_map & graph_spec_refined (for refined mode).

All heavy lifting follows Z1/examples/run_proper_sfc.py:
  CachedFedDataLoader(base_directory=..., cache_directory=...)
  load_single_source('Z1')
  DataProcessor.process_fed_data(z1_raw, 'Z1')


Fixed SFC core runner with correct Z1 series parsing based on Fed documentation.

Z1 Series Structure:
FA 15 30611 0 5 . Q
│  │  │     │ │ │ └── Frequency (Q=Quarterly, A=Annual)
│  │  │     │ │ └──── Calculation type (0,1,3=input; 5,6=calculated)
│  │  │     │ └────── Always 0 (digit 8)
│  │  │     └──────── Instrument code (5 digits, positions 3-7)
│  │  └─────────────── Sector code (2 digits, positions 1-2)
└───────────────────── Prefix (FL=Level, FU=Flow, FR=Revaluation, etc.)
"""

import argparse, json, sys
from pathlib import Path
import yaml
import pandas as pd
import numpy as np
import re

from src.data.cached_fed_data_loader import CachedFedDataLoader
from src.data.data_processor import DataProcessor
from src.utils.z1_series_interpreter import Z1Series
from src.alloc.graph_flow_allocator import allocate_flows_from_graph

# ---------- config & IO ----------

def load_cfg(path: str) -> dict:
    cfg_path = Path(path)
    with cfg_path.open("r", encoding="utf-8") as f:
        raw = yaml.safe_load(f) or {}
    if "sfc" not in raw:
        raise ValueError(f"{cfg_path} must contain an 'sfc' section.")
    sfc = raw["sfc"]
    req = ["base_dir","cache_dir","output_dir","date","instrument_map","flow_map"]
    missing = [k for k in req if not sfc.get(k)]
    if missing:
        raise ValueError(f"sfc config missing: {missing}")
    return sfc

# Correct regex pattern based on Fed documentation
_SERIES_RE = re.compile(
    r'^(?P<prefix>[A-Z]{2})'      # Prefix (FA, FL, FU, etc.)
    r'(?P<sector>\d{2})'           # Sector (2 digits)
    r'(?P<instrument>\d{5})'       # Instrument (5 digits)
    r'(?P<digit8>\d)'              # Digit 8 (always 0)
    r'(?P<calc_type>\d)'           # Digit 9 (calculation type)
    r'\.(?P<freq>[AQ])$'           # Frequency (.A or .Q)
)

# Alternative patterns for variations
_SERIES_RE_NO_DOT = re.compile(
    r'^(?P<prefix>[A-Z]{2})'
    r'(?P<sector>\d{2})'
    r'(?P<instrument>\d{5})'
    r'(?P<digit8>\d)'
    r'(?P<calc_type>\d)'
    r'(?P<freq>[AQ])$'
)

_SERIES_RE_SHORT = re.compile(
    r'^(?P<prefix>[A-Z]{2})'
    r'(?P<sector>\d{2})'
    r'(?P<instrument>\d+)'        # Variable length instrument
    r'\.(?P<freq>[AQ])$'
)

_ALLOWED_PREFIXES = {"FA","FL","FU","FV","FR","FC","FG","FI","FS","LA","LM","PC"}

def parse_z1_series(series_str: str):
    """
    Parse Z1 series mnemonic according to Fed documentation.
    Returns (prefix, sector, instrument) or None.
    """
    # Clean the series string
    series_str = str(series_str).strip().upper()
    
    # Try main pattern first
    m = _SERIES_RE.match(series_str)
    if m:
        prefix = m.group('prefix')
        if prefix in _ALLOWED_PREFIXES:
            return prefix, m.group('sector'), m.group('instrument')
    
    # Try without dot
    m = _SERIES_RE_NO_DOT.match(series_str)
    if m:
        prefix = m.group('prefix')
        if prefix in _ALLOWED_PREFIXES:
            return prefix, m.group('sector'), m.group('instrument')
    
    # Try shorter format
    m = _SERIES_RE_SHORT.match(series_str)
    if m:
        prefix = m.group('prefix')
        if prefix in _ALLOWED_PREFIXES:
            instrument = m.group('instrument')
            # Pad or truncate to 5 digits
            if len(instrument) > 5:
                instrument = instrument[:5]
            elif len(instrument) < 5:
                instrument = instrument.zfill(5)
            return prefix, m.group('sector'), instrument
    
    return None

def load_z1_panel(sfc: dict) -> pd.DataFrame:
    """Load and parse Z1 data with comprehensive debugging."""
    print("\n" + "="*60)
    print("LOADING Z1 DATA")
    print("="*60)
    
    loader = CachedFedDataLoader(
        base_directory=sfc["base_dir"],
        cache_directory=sfc["cache_dir"],
        start_year=1959,
        end_year=2025,  # Changed from default 2024 to 2025
        cache_expiry_days=30
    )
    
    z1_raw = loader.load_single_source('Z1')
    if z1_raw is None:
        raise ValueError("Failed to load Z1 data from cache/source")
    
    print(f"✓ Raw Z1 shape: {z1_raw.shape}")
    
    z1 = DataProcessor().process_fed_data(z1_raw, 'Z1')
    print(f"✓ Processed Z1 shape: {z1.shape}")
    
    # Sample series names for analysis
    print("\nSample series names from data:")
    sample_cols = list(z1.columns)[:20]
    for i, col in enumerate(sample_cols):
        parsed = parse_z1_series(col)
        if parsed:
            prefix, sector, instrument = parsed
            print(f"  {col:20} → {prefix} | Sector:{sector} | Instr:{instrument}")
        else:
            print(f"  {col:20} → [FAILED TO PARSE]")
    
    # Ensure index is named for melt
    if z1.index.name is None:
        z1.index.name = "date"
    
    # Wide -> long format
    df = z1.reset_index().melt(id_vars=["date"], var_name="series", value_name="value")
    df["date"] = pd.to_datetime(df["date"])
    
    print(f"\n✓ Long format: {df.shape[0]:,} rows")
    print(f"✓ Unique series: {df['series'].nunique():,}")
    
    # Parse all series
    print("\nParsing series codes...")
    parsed_results = df["series"].apply(parse_z1_series)
    
    # Count successful parses
    success_mask = parsed_results.notnull()
    print(f"\nParsing Results:")
    print(f"  ✓ Successful: {success_mask.sum():,} rows ({100*success_mask.mean():.1f}%)")
    print(f"  ✗ Failed: {(~success_mask).sum():,} rows ({100*(~success_mask).mean():.1f}%)")
    
    # Analyze failed parses
    if (~success_mask).any():
        failed_series = df.loc[~success_mask, "series"].unique()
        print(f"\nFailed series examples (first 10):")
        for s in failed_series[:10]:
            print(f"  - {s}")
        
        # Save debug info
        outdir = Path(sfc["output_dir"])
        outdir.mkdir(parents=True, exist_ok=True)
        debug_file = outdir / f"debug_unparsed_{pd.Timestamp.now():%Y%m%d_%H%M%S}.csv"
        pd.DataFrame({"series": failed_series}).to_csv(debug_file, index=False)
        print(f"\nComplete list saved to: {debug_file}")
    
    # Expand parsed tuples
    expanded = parsed_results.apply(
        lambda x: x if x is not None else (pd.NA, pd.NA, pd.NA)
    ).apply(pd.Series)
    expanded.columns = ["kind", "sector", "instrument"]
    
    # Create tidy dataframe
    tidy = pd.concat([df, expanded], axis=1)
    tidy = tidy.dropna(subset=["kind", "sector", "instrument"])
    
    # Analysis by series type
    print(f"\n" + "-"*40)
    print("SERIES BREAKDOWN BY TYPE:")
    print("-"*40)
    
    type_counts = tidy.groupby("kind").agg({
        "series": ["nunique", "count"]
    }).round(0).astype(int)
    type_counts.columns = ["Unique Series", "Total Rows"]
    
    for kind in sorted(tidy["kind"].unique()):
        unique = type_counts.loc[kind, "Unique Series"]
        total = type_counts.loc[kind, "Total Rows"]
        print(f"  {kind:3} : {unique:5,} series, {total:8,} rows")
    
    # Check specific date
    qdate = pd.to_datetime(sfc["date"])
    date_data = tidy[tidy["date"] == qdate]
    
    print(f"\n" + "-"*40)
    print(f"DATA AT ANALYSIS DATE: {qdate.date()}")
    print("-"*40)
    
    if len(date_data) == 0:
        print(f"⚠ WARNING: No data found for {qdate.date()}")
        available_dates = sorted(tidy["date"].unique())
        print(f"\nAvailable dates:")
        print(f"  First: {available_dates[0].date()}")
        print(f"  Last:  {available_dates[-1].date()}")
        print(f"\nRecent dates:")
        for d in available_dates[-5:]:
            count = len(tidy[tidy["date"] == d])
            print(f"    {d.date()}: {count:,} rows")
    else:
        for kind in ["FL", "FU", "FR", "FV", "FA"]:
            count = len(date_data[date_data["kind"] == kind])
            if count > 0:
                print(f"  {kind}: {count:,} rows")
    
    return tidy[["date", "series", "value", "kind", "sector", "instrument"]]

def load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ---------- SFC primitives ----------

def sfc_sign(instr: str, imap: dict) -> int:
    """Get sign for instrument based on side classification."""
    side = imap.get(instr, {}).get('side', 'asset')
    
    # Default unmapped to asset
    if instr not in imap:
        return -1
    
    if side == 'asset':
        return -1
    elif side == 'liability':
        return +1
    else:  # 'macro' or other
        return 0

def build_balance_sheet(tidy: pd.DataFrame, imap: dict, qdate: pd.Timestamp) -> pd.DataFrame:
    """Build balance sheet matrix from FL (level) series."""
    z = tidy[(tidy['kind'] == 'FL') & (tidy['date'] == qdate)].copy()
    
    print(f"\nBuilding Balance Sheet:")
    print(f"  Date: {qdate.date()}")
    print(f"  FL entries: {len(z):,}")
    
    if len(z) == 0:
        print("  ⚠ No FL data found!")
        return pd.DataFrame()
    
    mat = z.pivot_table(
        index='instrument', 
        columns='sector', 
        values='value', 
        aggfunc='sum'
    ).fillna(0.0)
    
    mat.insert(0, 'label', mat.index.map(lambda k: imap.get(k, {}).get('label', '')))
    mat['Total'] = mat.drop(columns=['label']).sum(axis=1)
    
    print(f"  Matrix shape: {mat.shape}")
    return mat.sort_index()

def build_transactions_financial(tidy: pd.DataFrame, imap: dict, qdate: pd.Timestamp) -> pd.DataFrame:
    """Build financial transactions matrix from FU (flow) series."""
    z = tidy[(tidy['kind'] == 'FU') & (tidy['date'] == qdate)].copy()
    
    print(f"\nBuilding Transaction Matrix:")
    print(f"  Date: {qdate.date()}")
    print(f"  FU entries: {len(z):,}")
    
    if len(z) == 0:
        print("  ⚠ No FU data found!")
        
        # Additional debugging
        all_fu = tidy[tidy['kind'] == 'FU']
        if len(all_fu) > 0:
            print(f"\n  FU data exists in other periods:")
            fu_dates = all_fu['date'].value_counts().head(5)
            for date, count in fu_dates.items():
                print(f"    {date.date()}: {count:,} rows")
        
        return pd.DataFrame()
    
    # Check instrument mapping
    print(f"  Unique instruments: {z['instrument'].nunique()}")
    unmapped = z[~z['instrument'].isin(imap.keys())]['instrument'].unique()
    
    if len(unmapped) > 0:
        print(f"  ⚠ Unmapped instruments: {len(unmapped)}")
        print(f"    Examples: {list(unmapped[:5])}")
    
    # Apply signs
    z['signed_value'] = z.apply(
        lambda row: sfc_sign(row['instrument'], imap) * row['value'],
        axis=1
    )
    
    # Check sign application
    non_zero = (z['signed_value'] != 0).sum()
    zeros = (z['signed_value'] == 0).sum()
    print(f"  Sign application:")
    print(f"    Non-zero: {non_zero:,}")
    print(f"    Zero: {zeros:,}")
    
    mat = z.pivot_table(
        index='instrument',
        columns='sector',
        values='signed_value',
        aggfunc='sum'
    ).fillna(0.0)
    
    mat.insert(0, 'label', mat.index.map(lambda k: imap.get(k, {}).get('label', '')))
    mat['Total'] = mat.drop(columns=['label']).sum(axis=1)
    
    print(f"  Matrix shape: {mat.shape}")
    return mat

def add_macro_flows(trans_mat: pd.DataFrame, tidy: pd.DataFrame, fmap: dict, qdate: pd.Timestamp) -> pd.DataFrame:
    """Add macro flows to transaction matrix."""
    if trans_mat.empty:
        print("\n⚠ Cannot add macro flows to empty transaction matrix")
        return trans_mat
    
    print(f"\nAdding Macro Flows:")
    
    sector_cols = [c for c in trans_mat.columns if c not in ('label', 'Total')]
    rows = []
    
    for code, meta in fmap.items():
        blk = tidy[(tidy['date'] == qdate) & (tidy['instrument'] == code)]
        if blk.empty:
            continue
            
        val = blk['value'].sum()
        row = {c: 0.0 for c in sector_cols}
        
        sbs = meta.get('sign_by_sector', {})
        if sbs:
            for sec, s in sbs.items():
                if sec in row:
                    row[sec] += s * val
        else:
            tos = [s for s in meta.get('to', []) if s in row]
            frs = [s for s in meta.get('from', []) if s in row]
            n_to = len(tos) or 1
            n_fr = len(frs) or 1
            for sec in tos:
                row[sec] += val / n_to
            for sec in frs:
                row[sec] -= val / n_fr
                
        row['label'] = meta.get('label', f'Flow {code}')
        rows.append((code, row))
    
    print(f"  Added {len(rows)} macro flows")
    
    if not rows:
        return trans_mat
    
    add_df = pd.DataFrame.from_dict({k: v for k, v in rows}, orient='index')
    for c in sector_cols:
        if c not in add_df.columns:
            add_df[c] = 0.0
    add_df = add_df[['label'] + sector_cols]
    add_df['Total'] = add_df[sector_cols].sum(axis=1)
    
    return pd.concat([trans_mat, add_df], axis=0)

def recon_stock_flow(tidy: pd.DataFrame, qdate: pd.Timestamp) -> pd.DataFrame:
    """Reconcile stocks and flows."""
    print(f"\nStock-Flow Reconciliation:")
    
    t = tidy[tidy['date'] == qdate]
    prev_dates = tidy[tidy['date'] < qdate]['date'].unique()
    
    if len(prev_dates) == 0:
        print("  ⚠ No prior period for reconciliation")
        return pd.DataFrame()
    
    prev_date = max(prev_dates)
    prev = tidy[tidy['date'] == prev_date]
    
    print(f"  Period: {prev_date.date()} → {qdate.date()}")
    
    recon = []
    for (sector, instrument) in t[['sector', 'instrument']].drop_duplicates().values:
        # Current and previous levels
        fl_curr = t[(t['kind'] == 'FL') & (t['sector'] == sector) & 
                   (t['instrument'] == instrument)]['value'].sum()
        fl_prev = prev[(prev['kind'] == 'FL') & (prev['sector'] == sector) & 
                       (prev['instrument'] == instrument)]['value'].sum()
        
        # Flows
        fu = t[(t['kind'] == 'FU') & (t['sector'] == sector) & 
               (t['instrument'] == instrument)]['value'].sum()
        fr = t[(t['kind'] == 'FR') & (t['sector'] == sector) & 
               (t['instrument'] == instrument)]['value'].sum()
        fv = t[(t['kind'] == 'FV') & (t['sector'] == sector) & 
               (t['instrument'] == instrument)]['value'].sum()
        
        dfl = fl_curr - fl_prev
        gap = dfl - (fu + fr + fv)
        
        if abs(dfl) > 1e-6 or abs(fu) > 1e-6:
            recon.append({
                'sector': sector,
                'instrument': instrument,
                'dFL': dfl,
                'FU': fu,
                'FR': fr,
                'FV': fv,
                'Gap': gap
            })
    
    result = pd.DataFrame(recon)
    print(f"  Reconciliation entries: {len(result):,}")
    
    if len(result) > 0:
        gap_stats = result['Gap'].abs()
        print(f"  Gap statistics:")
        print(f"    Mean: {gap_stats.mean():.2f}")
        print(f"    Max:  {gap_stats.max():.2f}")
        print(f"    >1e-6: {(gap_stats > 1e-6).sum():,} entries")
    
    return result

# ---------- execution modes ----------

def run_baseline(sfc: dict, tidy: pd.DataFrame):
    """Run baseline mode."""
    print("\n" + "="*60)
    print("BASELINE MODE EXECUTION")
    print("="*60)
    
    imap = load_json(sfc["instrument_map"])
    fmap = load_json(sfc["flow_map"])
    qdate = pd.to_datetime(sfc["date"])
    outdir = Path(sfc["output_dir"])
    outdir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nConfiguration:")
    print(f"  Analysis date: {qdate.date()}")
    print(f"  Output directory: {outdir}")
    print(f"  Instruments mapped: {len(imap)}")
    print(f"  Flow mappings: {len(fmap)}")
    
    # Build matrices
    bs = build_balance_sheet(tidy, imap, qdate)
    tf_fin = build_transactions_financial(tidy, imap, qdate)
    
    if not tf_fin.empty:
        tf_full = add_macro_flows(tf_fin, tidy, fmap, qdate)
    else:
        tf_full = tf_fin
        print("\n⚠ Transaction matrix is empty - skipping macro flows")
    
    rc = recon_stock_flow(tidy, qdate)
    
    # Save outputs
    tag = qdate.date().isoformat()
    
    print(f"\n" + "="*60)
    print("SAVING OUTPUTS")
    print("="*60)
    
    if not bs.empty:
        bs_file = outdir / f'sfc_balance_sheet_{tag}.csv'
        bs.to_csv(bs_file)
        print(f"  ✓ Balance sheet: {bs.shape} → {bs_file.name}")
    
    if not tf_full.empty:
        tf_file = outdir / f'sfc_transactions_{tag}.csv'
        tf_full.to_csv(tf_file)
        print(f"  ✓ Transactions: {tf_full.shape} → {tf_file.name}")
    
    if not rc.empty:
        rc_file = outdir / f'sfc_recon_{tag}.csv'
        rc.to_csv(rc_file, index=False)
        print(f"  ✓ Reconciliation: {rc.shape} → {rc_file.name}")
    
    print(f"\nAnalysis complete!")

# Stub functions for other modes (implement as needed)
def run_fwtw(sfc: dict, tidy: pd.DataFrame):
    print("FWTW mode not yet implemented")

def run_graph(sfc: dict, tidy: pd.DataFrame):
    print("Graph mode not yet implemented")

def run_refined(sfc: dict, tidy: pd.DataFrame):
    print("Refined mode not yet implemented")

# ---------- main ----------

def main():
    ap = argparse.ArgumentParser(description="SFC core runner")
    ap.add_argument("mode", choices=["baseline", "fwtw", "graph", "refined"])
    ap.add_argument("--config", default="config/proper_sfc_config.yaml")
    args = ap.parse_args()

    try:
        sfc = load_cfg(args.config)
        tidy = load_z1_panel(sfc)
        
        if args.mode == "baseline":
            run_baseline(sfc, tidy)
        elif args.mode == "fwtw":
            run_fwtw(sfc, tidy)
        elif args.mode == "graph":
            run_graph(sfc, tidy)
        elif args.mode == "refined":
            run_refined(sfc, tidy)
            
    except Exception as e:
        print(f"\n❌ ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
